{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bit4bd693e9780a48afbf8d298023c65066",
   "display_name": "Python 3.7.3 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASIC IMPORTS OF ALL LIBRARIES\n",
    "'''\n",
    "Glove Imports are 300-dim from GENSIM\n",
    "Pickle saves all the datasets\n",
    "'''\n",
    "import pygraphviz\n",
    "from nltk import pos_tag\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "from entity_groups import *\n",
    "from graphviz import Source\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist \n",
    "import gensim.downloader as api\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "word_vectors = api.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# junk verbs that are generalizably illegal\n",
    "junk_rel = [\"be\",\"have\",\"do\",\"let\",\"\",\"make\",\"tell\",\"say\",\"describe\",\"decide\",\"want\",\"name\",\"know\",\"think\",\"try\",\"become\",\"oneday\",\"put\",\"come\",'see', 'need', 'look', 'help', 'come', 'take', 'get', 'put', 'pick', 'turn', 'go', 'stand', 'give', 'notice', 'use',\"get\",\"start\"]\n",
    "\n",
    "# hypothetical words that are generally useless / qualifiers\n",
    "junk_words = [\"would\",\"could\",\"should\",\"maybe\",\"perhaps\",\"think\",\"might\",\"assume\",\"claim\",\"this\"]\n",
    "\n",
    "# negative sentences\n",
    "junk_words.extend(['n\\'t',\"not\"])\n",
    "\n",
    "# perspective actants that are pointless\n",
    "junk = [\"you\",\"i\",\"we\",\"the\",\"it\",\"he\",\"she\",\"steinbeck\",\"people\",\"author\",\n",
    "        \"book\",\"me\",\"steinback\",\"him\",\"her\",\"their\",\"this\",\"\",\"shelley\",\"mary\",\"harper\",\"lee\",\"tolkien\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE = \"0417\"\n",
    "file_name = \"GDELT/\"\n",
    "text = \"GDELT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "entity_file = \"../Covid_19_Data/Extracted_Data/OS_News/\" + \"df_ent_final_ranking.csv\"\n",
    "S_use = {}\n",
    "# fill this in\n",
    "\n",
    "places = []\n",
    "ent_file = pd.read_csv(entity_file)\n",
    "for row in ent_file.head(n=20).iterrows():\n",
    "    if row[1]['type'] == \"GPE\":\n",
    "        places.append(row[1]['entity'])\n",
    "    S_use[row[1]['entity']] = row[1]['entity']\n",
    "    \n",
    "actants_5g = [\"5g\",\"radio\",\"transmitters\",\"towers\",\"radiation\",\"army\",\"waves\",\"conspiracy\"]\n",
    "for a in actants_5g:\n",
    "    S_use[a] = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Extracted_0416/\"\n",
    "date_start = \"20200101000001\"\n",
    "date_end = \"20200410000001\"\n",
    "date = date_start\n",
    "results = \"Results0417\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_traj(file_name_re):\n",
    "    trajectories = []\n",
    "    same_post = 0\n",
    "    traj = [\"START\"]\n",
    "\n",
    "    with open(file_name_re) as csvfile:\n",
    "        readCSV = csv.reader(csvfile, delimiter=',')\n",
    "\n",
    "        for row in readCSV:\n",
    "            FLAG = 0\n",
    "            sentence_num = row[1]\n",
    "            sub_r = \"\"\n",
    "            obj_r = \"\"\n",
    "            sub_verb = \"\"\n",
    "\n",
    "            if hypothetical_sentence(row[2]):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                sub = re.search(r\"\\{(\\w+)\\}\", row[3]).group(1).lower()\n",
    "                rel = wnl.lemmatize(re.search(r\"\\{(\\w+)\\}\", row[4]).group(1).lower(), pos='v')\n",
    "                obj = re.search(r\"\\{(\\w+)\\}\", row[5]).group(1).lower()\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            words = row[2].split()\n",
    "            rel_words = []\n",
    "\n",
    "            if rel == None or obj == None or sub == None:\n",
    "                continue\n",
    "            \n",
    "            if same_post == row[0]:\n",
    "                pass\n",
    "            else:\n",
    "                same_post = row[0]\n",
    "                traj.append(\"TERMINATE\")\n",
    "                trajectories.append(traj)\n",
    "                traj = [\"START\"]\n",
    "\n",
    "            try:\n",
    "                sub_r = S_use[sub]\n",
    "            except:\n",
    "                sub_r = sub\n",
    "                FLAG += 1\n",
    "            \n",
    "            try:\n",
    "                obj_r = S_use[obj]\n",
    "            except:\n",
    "                obj_r = obj\n",
    "                FLAG += 1\n",
    "\n",
    "            sub_verb = \"\"\n",
    "\n",
    "            try:\n",
    "                sub_verb = C_use[rel]\n",
    "            except:\n",
    "                sub_verb = rel\n",
    "\n",
    "            if sub_r == obj_r or sub_r in junk or obj_r in junk or sub_verb in junk_rel \\\n",
    "                                    or FLAG > 0 or sub_r.isdigit() or obj_r.isdigit():\n",
    "                continue\n",
    "\n",
    "            traj.append(sub_verb)\n",
    "            traj.append(sub_r + \"_\" + obj_r + \"_\" + sentence_num)\n",
    "\n",
    "    # if we missed out on last post\n",
    "    if traj != \"START\":\n",
    "        traj.append(\"TERMINATE\")\n",
    "        trajectories.append(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mat(trajectories):\n",
    "    rev_dict = {}\n",
    "    precedence_matrix = np.zeros((50000,50000))\n",
    "    dictionary_of_labels = {\"START\":0, \"TERMINATE\":1}\n",
    "    counter_of_labels = defaultdict(int)\n",
    "    max_rows = 1\n",
    "    max_cols = 1\n",
    "    counter = 2\n",
    "\n",
    "    for trajectory in trajectories:\n",
    "\n",
    "        triplets = [\"START\"]\n",
    "        index = 1\n",
    "\n",
    "        while index < len(trajectory)-2:\n",
    "            triplets.append(trajectory[index] + '_' + trajectory[index + 1])\n",
    "            index += 2\n",
    "\n",
    "        triplets.append(\"TERMINATE\")\n",
    "\n",
    "        for key,_ in enumerate(triplets[1:-1]):\n",
    "            removed_triplets = triplets[key+1].split(\"_\")\n",
    "            if len(removed_triplets) > 1:\n",
    "                subject_ = removed_triplets[1]\n",
    "                object_ = removed_triplets[2]\n",
    "                if subject_ not in dictionary_of_labels:\n",
    "                    dictionary_of_labels[subject_] = counter\n",
    "                    counter += 1\n",
    "                if object_ not in dictionary_of_labels:\n",
    "                    dictionary_of_labels[object_] = counter\n",
    "                    counter += 1\n",
    "        \n",
    "        triplet_temp = []\n",
    "\n",
    "        # find uniques\n",
    "        for trip in triplets:\n",
    "            try:\n",
    "                removed_triplet = trip.rsplit(\"_\",1)[0]\n",
    "            except:\n",
    "                removed_triplet = trip\n",
    "\n",
    "            if removed_triplet not in [t.rsplit(\"_\",1)[0] for t in triplet_temp]:\n",
    "                triplet_temp.append(trip)\n",
    "\n",
    "        triplets = triplet_temp\n",
    "\n",
    "        # for trip in triplets:\n",
    "        #     counter_of_labels[trip] += 1\n",
    "\n",
    "        alpha = 1.0\n",
    "\n",
    "        for key in range(1,len(triplets)):\n",
    "            removed_triplets = triplets[key].split(\"_\")\n",
    "            try:\n",
    "                subject_ = removed_triplets[1]\n",
    "                object_ = removed_triplets[2]\n",
    "                precedence_matrix[dictionary_of_labels[subject_]]\\\n",
    "                                            [dictionary_of_labels[object_]] += 1.0\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "    precedence_matrix = precedence_matrix[:counter,:counter]\n",
    "    for key in dictionary_of_labels:\n",
    "        rev_dict[dictionary_of_labels[key]] = key\n",
    "\n",
    "    return precedence_matrix, dictionary_of_labels, rev_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fig(trajectories,label):    \n",
    "    while i < len(trajectories):\n",
    "        sub_t = trajectories\n",
    "        mat, dic, rev_dict = find_mat(sub_t)\n",
    "\n",
    "        G = nx.from_numpy_matrix(mat, create_using=nx.DiGraph)\n",
    "        DISP_THRESH = 2\n",
    "\n",
    "        bb = {}\n",
    "        for index, val in np.ndenumerate(mat):\n",
    "            if val > 1:\n",
    "                bb[index] = {\"color\":\"red\", \"penwidth\":2}\n",
    "            else:\n",
    "                bb[index] = {\"color\":\"blue\"}\n",
    "        nx.set_edge_attributes(G, bb)\n",
    "\n",
    "        bb1 = {}\n",
    "        for j in range(mat.shape[0]):\n",
    "            if rev_dict[j] in actants_5g:\n",
    "                bb1[j] = {\"color\":\"red\", \"penwidth\":4}\n",
    "            else:\n",
    "                bb1[j] = {\"color\":\"blue\"}\n",
    "\n",
    "        nx.set_node_attributes(G, bb1)\n",
    "\n",
    "        bb2 = {}\n",
    "        for j in range(mat.shape[0]):\n",
    "            if rev_dict[j] in places:\n",
    "                bb2[j] = {\"color\":\"green\", \"penwidth\":4}\n",
    "            else:\n",
    "                bb2[j] = {\"color\":\"blue\"}\n",
    "        nx.set_node_attributes(G, bb2)\n",
    "\n",
    "        H = nx.relabel_nodes(G, rev_dict)\n",
    "        H.remove_nodes_from(list(nx.isolates(H)))\n",
    "        A = nx.nx_agraph.to_agraph(H)\n",
    "\n",
    "        A.layout('dot')\n",
    "        A.draw(results + \"Actant_TimeSeries/step\" + str(label) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "def to_date(DATE):\n",
    "    return datetime.strptime(DATE, '%Y%m%d%H%M%S')\n",
    "\n",
    "def from_date(DATE):\n",
    "    return DATE.strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "def create_range(DATE):\n",
    "    list_dates = []\n",
    "    for i in range(5):\n",
    "        list_dates.append(DATE + timedelta(days = i))\n",
    "    return list_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Extracted_0416/20200101000001into_relex_final_relations_-1.csv'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-faf0f041d4ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdate_margin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mner_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfrom_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"into_relex_final_relations_-1.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtrajs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_traj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mner_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mplot_fig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_real\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-b9c076cd144b>\u001b[0m in \u001b[0;36mextract_traj\u001b[0;34m(file_name_re)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtraj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"START\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name_re\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mreadCSV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Extracted_0416/20200101000001into_relex_final_relations_-1.csv'"
     ]
    }
   ],
   "source": [
    "date_real = to_date(date)\n",
    "date_end_real = to_date(date_end)\n",
    "\n",
    "while date_real <= date_end_real:    \n",
    "    trajs = []\n",
    "    date_margin = create_range(date_real)\n",
    "\n",
    "    for d in date_margin:\n",
    "        ner_file = path + from_date(d) + \"/into_relex_final_relations_-1.csv\"\n",
    "        trajs.extend(extract_traj(ner_file))\n",
    "\n",
    "    plot_fig(traj, from_date(date_real))\n",
    "    date_real += timedelta(days = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
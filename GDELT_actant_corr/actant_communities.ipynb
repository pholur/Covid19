{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bit4bd693e9780a48afbf8d298023c65066",
   "display_name": "Python 3.7.3 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASIC IMPORTS OF ALL LIBRARIES\n",
    "'''\n",
    "Glove Imports are 300-dim from GENSIM\n",
    "Pickle saves all the datasets\n",
    "'''\n",
    "import pygraphviz\n",
    "from nltk import pos_tag\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "from graphviz import Source\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist \n",
    "import gensim.downloader as api\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "from sklearn.preprocessing import normalize\n",
    "from datetime import datetime, timedelta\n",
    "import datetime\n",
    "\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# junk verbs that are generalizably illegal\n",
    "junk_rel = [\"be\",\"have\",\"do\",\"let\",\"\",\"make\",\"tell\",\"say\",\"describe\",\"decide\",\"want\",\"name\",\"know\",\"think\",\"try\",\"become\",\"oneday\",\"put\",\"come\",'see', 'need', 'look', 'help', 'come', 'take', 'get', 'put', 'pick', 'turn', 'go', 'stand', 'give', 'notice', 'use',\"get\",\"start\"]\n",
    "\n",
    "# hypothetical words that are generally useless / qualifiers\n",
    "junk_words = [\"would\",\"could\",\"should\",\"maybe\",\"perhaps\",\"think\",\"might\",\"assume\",\"claim\",\"this\"]\n",
    "\n",
    "# negative sentences\n",
    "junk_words.extend(['n\\'t',\"not\"])\n",
    "\n",
    "# perspective actants that are pointless\n",
    "junk = [\"you\",\"i\",\"we\",\"the\",\"it\",\"he\",\"she\",\"steinbeck\",\"people\",\"author\",\n",
    "        \"book\",\"me\",\"steinback\",\"him\",\"her\",\"their\",\"this\",\"\",\"shelley\",\"mary\",\"harper\",\"lee\",\"tolkien\",\"bjp\",\"delhi\",\"man\",\"men\",\"giannulli\",\"loughlin\",\"singh\",\"ryan\",\"way\",\"lot\",\"guy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "entity_file = \"NER_list.csv\"\n",
    "S_use = {}\n",
    "# fill this in\n",
    "\n",
    "places = []\n",
    "people = []\n",
    "orgs = []\n",
    "\n",
    "ent_file = pd.read_csv(entity_file)\n",
    "for row in ent_file.head(100).iterrows():\n",
    "     S_use[row[1]['entity']] = row[1]['entity']\n",
    "\n",
    "for row in ent_file.iterrows():\n",
    "    if row[1]['type'] == \"PERSON\":\n",
    "        people.append(row[1]['entity'])\n",
    "        # S_use[row[1]['entity']] = row[1]['entity']\n",
    "\n",
    "for row in ent_file.iterrows():\n",
    "    if row[1]['type'] == \"GPE\":\n",
    "        places.append(row[1]['entity'])\n",
    "        # S_use[row[1]['entity']] = row[1]['entity']\n",
    "\n",
    "for row in ent_file.iterrows():\n",
    "    if row[1]['type'] == \"ORG\":\n",
    "        orgs.append(row[1]['entity'])\n",
    "        # S_use[row[1]['entity']] = row[1]['entity']\n",
    "\n",
    "ads = [\"youtube\",\"twitter\",\"facebook\",\"reddit\",\"4chan\",\"8chan\",\"instagram\"]\n",
    "hardcode = [\"5g\",\"radio\",\"waves\",\"jews\",\"army\",\"nanowires\",\"alien\",\"aliens\",\"towers\",\"bleach\",\"garlic\"]\n",
    "# for a in ads:\n",
    "#     S_use[a] = a\n",
    "\n",
    "# for a in hardcode:\n",
    "#     S_use[a] = a\n",
    "    \n",
    "# S_use[\"alex\"] = \"alex jones\"\n",
    "# S_use[\"jones\"] = \"alex jones\"\n",
    "# S_use[\"virus\"] = \"covid19\"\n",
    "# S_use[\"coronavirus\"] = \"covid19\"\n",
    "# S_use[\"theory\"] = \"conspiracy theory\"\n",
    "# S_use[\"theories\"] = \"conspiracy theory\"\n",
    "# S_use[\"conspiracy\"] = \"conspiracy theory\"\n",
    "# S_use[\"trumps\"] = S_use[\"trump\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"ExtractedGDELTData/\"\n",
    "date_start = \"20200101000001\"\n",
    "date_end = \"20200410000001\"\n",
    "#date_end = \"20200102000001\"\n",
    "date = date_start\n",
    "results = \"Results0424\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_traj(file_name_re, S, dd):\n",
    "    trajectories = []\n",
    "    same_post = 0\n",
    "    traj = [\"START\"]\n",
    "\n",
    "    with open(file_name_re) as csvfile:\n",
    "        readCSV = csv.reader(csvfile, delimiter=',')\n",
    "\n",
    "        for row in readCSV:\n",
    "            FLAG = 0\n",
    "            sentence_num = row[1]\n",
    "            sub_r = \"\"\n",
    "            obj_r = \"\"\n",
    "            sub_verb = \"\"\n",
    "\n",
    "            sub = \"\"\n",
    "            obj = \"\"\n",
    "            rel = \"\"\n",
    "            try:\n",
    "                sub = re.search(r\"\\{(\\w+)\\}\", row[3]).group(1).lower()\n",
    "                rel = wnl.lemmatize(re.search(r\"\\{(\\w+)\\}\", row[4]).group(1).lower(), pos='v')\n",
    "                obj = re.search(r\"\\{(\\w+)\\}\", row[5]).group(1).lower()\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            words = row[2].split()\n",
    "            rel_words = []\n",
    "\n",
    "\n",
    "            if rel == None or obj == None or sub == None:\n",
    "                continue\n",
    "            \n",
    "            if same_post == row[0]:\n",
    "                pass\n",
    "            else:\n",
    "                same_post = row[0]\n",
    "                traj.append(\"TERMINATE\")\n",
    "                trajectories.append(traj)\n",
    "                traj = [\"START\"]\n",
    "\n",
    "            try:\n",
    "                sub_r = S[sub]\n",
    "            except:\n",
    "                sub_r = sub\n",
    "                FLAG += 1\n",
    "            \n",
    "            try:\n",
    "                obj_r = S[obj]\n",
    "            except:\n",
    "                obj_r = obj\n",
    "                FLAG += 1\n",
    "\n",
    "            sub_verb = \"\"\n",
    "            sub_verb = rel\n",
    "\n",
    "            if sub_r == obj_r or sub_r in junk or obj_r in junk or sub_verb in junk_rel \\\n",
    "                                    or FLAG > 0 or sub_r.isdigit() or obj_r.isdigit():\n",
    "                continue\n",
    "            if sub_r == \"gates\":\n",
    "                print(S['gates'])\n",
    "            if sub_r == \"bill gates\" or obj_r == \"bill gates\":\n",
    "                dic = {\"date\":[dd],\"subject_phrase\":[row[3]],\"rel_phrase\":[row[4]],\"object_phrase\":[row[5]]}\n",
    "                df = pd.DataFrame(dic)\n",
    "                with open(results + '/bill_gates.csv', 'a') as f:\n",
    "                    df.to_csv(f, header=False)\n",
    "\n",
    "            traj.append(sub_verb)\n",
    "            traj.append(sub_r + \"_\" + obj_r + \"_\" + sentence_num)\n",
    "\n",
    "    # if we missed out on last post\n",
    "    if traj != \"START\":\n",
    "        traj.append(\"TERMINATE\")\n",
    "        trajectories.append(traj)\n",
    "    return trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mat(trajectories):\n",
    "    rev_dict = {}\n",
    "    precedence_matrix = np.zeros((50000,50000))\n",
    "    dictionary_of_labels = {\"START\":0, \"TERMINATE\":1}\n",
    "    counter_of_labels = defaultdict(int)\n",
    "    max_rows = 1\n",
    "    max_cols = 1\n",
    "    counter = 2\n",
    "\n",
    "    for trajectory in trajectories:\n",
    "\n",
    "        triplets = [\"START\"]\n",
    "        index = 1\n",
    "\n",
    "        while index < len(trajectory)-2:\n",
    "            triplets.append(trajectory[index] + '_' + trajectory[index + 1])\n",
    "            index += 2\n",
    "\n",
    "        triplets.append(\"TERMINATE\")\n",
    "\n",
    "        for key,_ in enumerate(triplets[1:-1]):\n",
    "            removed_triplets = triplets[key+1].split(\"_\")\n",
    "            if len(removed_triplets) > 1:\n",
    "                subject_ = removed_triplets[1]\n",
    "                object_ = removed_triplets[2]\n",
    "                if subject_ not in dictionary_of_labels:\n",
    "                    dictionary_of_labels[subject_] = counter\n",
    "                    counter += 1\n",
    "                if object_ not in dictionary_of_labels:\n",
    "                    dictionary_of_labels[object_] = counter\n",
    "                    counter += 1\n",
    "        \n",
    "        triplet_temp = []\n",
    "\n",
    "        # find uniques\n",
    "        for trip in triplets:\n",
    "            try:\n",
    "                removed_triplet = trip.rsplit(\"_\",1)[0]\n",
    "            except:\n",
    "                removed_triplet = trip\n",
    "\n",
    "            if removed_triplet not in [t.rsplit(\"_\",1)[0] for t in triplet_temp]:\n",
    "                triplet_temp.append(trip)\n",
    "\n",
    "        triplets = triplet_temp\n",
    "\n",
    "        # for trip in triplets:\n",
    "        #     counter_of_labels[trip] += 1\n",
    "\n",
    "        alpha = 1.0\n",
    "\n",
    "        for key in range(1,len(triplets)):\n",
    "            removed_triplets = triplets[key].split(\"_\")\n",
    "            try:\n",
    "                subject_ = removed_triplets[1]\n",
    "                object_ = removed_triplets[2]\n",
    "                precedence_matrix[dictionary_of_labels[subject_]]\\\n",
    "                                            [dictionary_of_labels[object_]] += 1.0\n",
    "                precedence_matrix[dictionary_of_labels[object_]]\\\n",
    "                                            [dictionary_of_labels[subject_]] += 1.0\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "    precedence_matrix = precedence_matrix[:counter,:counter]\n",
    "    for key in dictionary_of_labels:\n",
    "        rev_dict[dictionary_of_labels[key]] = key\n",
    "\n",
    "    return precedence_matrix, dictionary_of_labels, rev_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_partitions(G, rev_dict, date_):\n",
    "    list_of_comms = greedy_modularity_communities(G)\n",
    "    total_l = []\n",
    "    with open(results + \"/communities.txt\",\"a+\") as f:\n",
    "        for l in list_of_comms:\n",
    "            temp_list = []\n",
    "            for i in l:\n",
    "                if i not in [0,1]:\n",
    "                    temp_list.append(rev_dict[i])\n",
    "\n",
    "            if temp_list != []:\n",
    "                str_l = ', '.join(temp_list)\n",
    "                total_l.append(\"{\" + str_l + \"}\")\n",
    "        final_str = \", \".join(total_l) + \"\\n\"\n",
    "        f.write(date_ + \" -> \" + final_str)\n",
    "\n",
    "\n",
    "def save_simple_paths(G, dic):\n",
    "    w1 = \"coronavirus\"\n",
    "    w2 = \"conspiracy theory\"\n",
    "    with open(results + \"/simple_path.txt\",\"a+\") as f:\n",
    "        if w1 in dic and w2 in dic:\n",
    "            f.write(str(len(list(nx.all_simple_paths(G, source=dic[w1], target=dic[w2], cutoff = 2)))))\n",
    "            f.write(\"\\n\")\n",
    "        else:\n",
    "            f.write(str(0))\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fig(trajectories,label):    \n",
    "    sub_t = trajectories\n",
    "    mat, dic, rev_dict = find_mat(sub_t)\n",
    "    try:\n",
    "        mat = normalize(mat, 'l1')\n",
    "        for i in range(0, mat.shape[0]):\n",
    "            for j in range(0, mat.shape[1]):\n",
    "                if mat[i][j] > mat[j][i]:\n",
    "                    mat[j][i] = 0\n",
    "\n",
    "                elif mat[i][j] < mat[j][i]:\n",
    "                    mat[i][j] = 0\n",
    "\n",
    "                elif mat[i][j] == mat[j][i] and i != j and mat[j][i] != 0:\n",
    "                    mat[j][i] = 0\n",
    "\n",
    "        G = nx.from_numpy_matrix(mat, create_using=nx.Graph)\n",
    "        save_partitions(G, rev_dict, label[4:8])\n",
    "    except:\n",
    "        G = nx.from_numpy_matrix(mat, create_using=nx.Graph)\n",
    "\n",
    "    DISP_THRESH = 2\n",
    "\n",
    "    bb = {}\n",
    "    for index, val in np.ndenumerate(mat):\n",
    "        if val > 1:\n",
    "            bb[index] = {\"color\":\"red\", \"penwidth\":2}\n",
    "        else:\n",
    "            bb[index] = {\"color\":\"blue\", \"penwidth\":0.5}\n",
    "    nx.set_edge_attributes(G, bb)\n",
    "\n",
    "    bb1 = {}\n",
    "    for j in range(mat.shape[0]):\n",
    "        if rev_dict[j] in places:\n",
    "            bb1[j] = {\"color\":\"green\", \"penwidth\":4}\n",
    "        if rev_dict[j] in people:\n",
    "            bb1[j] = {\"color\":\"yellow\", \"penwidth\":4}\n",
    "        if rev_dict[j] in orgs:\n",
    "            bb1[j] = {\"color\":\"black\", \"penwidth\":4}\n",
    "        if rev_dict[j] in ads:\n",
    "            bb1[j] = {\"color\":\"red\", \"penwidth\":4}\n",
    "        if rev_dict[j] in hardcode:\n",
    "            bb1[j] = {\"color\":\"pink\", \"penwidth\":4}\n",
    "        if rev_dict[j] == \"coronavirus\":\n",
    "            bb1[j] = {\"color\":\"orange\", \"penwidth\":6}\n",
    "        if rev_dict[j] == \"conspiracy theory\":\n",
    "            bb1[j] = {\"color\":\"orange\", \"penwidth\":6}\n",
    "\n",
    "    nx.set_node_attributes(G, bb1)\n",
    "    save_simple_paths(G, dic)\n",
    "    H = nx.relabel_nodes(G, rev_dict)\n",
    "\n",
    "    H.remove_nodes_from(list(nx.isolates(H)))\n",
    "    A = nx.nx_agraph.to_agraph(H)\n",
    "\n",
    "    A.layout('fdp')\n",
    "    A.draw(results + \"/Actant_TimeSeries/step\" + str(label) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def to_date(DATE):\n",
    "    return datetime.strptime(DATE, '%Y%m%d%H%M%S')\n",
    "\n",
    "def from_date(DATE):\n",
    "    return DATE.strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "def create_range(DATE):\n",
    "    list_dates = []\n",
    "    for i in range(5):\n",
    "        list_dates.append(DATE + timedelta(days = i))\n",
    "    return list_dates\n",
    "\n",
    "def create_dict(string_of_words):\n",
    "    S = {}\n",
    "    words = string_of_words.split(\", \")\n",
    "    for word in words:\n",
    "        # REMOVE FOR VERSION 2\n",
    "        if word in [\"mr\",\"gandhi\",\"thakur\",\"pulwama\"]:\n",
    "            continue\n",
    "\n",
    "        S[word] = word\n",
    "        if word in [\"theories\",\"theory\",\"conspiracy\"]:\n",
    "            S[\"theory\"] = \"conspiracy theory\"\n",
    "            S[\"theories\"] = \"conspiracy theory\"\n",
    "            S[\"conspiracy\"] = \"conspiracy theory\"\n",
    "        if word == \"alex\" or word == \"jones\":\n",
    "            S[\"alex\"] = \"alex jones\"\n",
    "            S[\"jones\"] = \"alex jones\"\n",
    "        if word == \"trump\" or word == \"trumps\":\n",
    "            S[\"trump\"] = \"donald trump\"\n",
    "            S[\"trumps\"] = \"donald trump\"\n",
    "        if word == \"virus\" or word == \"coronavirus\" or word == \"covid19\":\n",
    "            S[\"virus\"] = \"coronavirus\"\n",
    "            S[\"coronavirus\"] = \"coronavirus\"\n",
    "            S[\"covid19\"] = \"coronavirus\"\n",
    "        if word == \"bill\" or word == \"gates\":\n",
    "            S[\"bill\"] = \"bill gates\"\n",
    "            S[\"gates\"] = \"bill gates\"\n",
    "    return S\n",
    "\n",
    "def recheck(S):\n",
    "    for word in S:\n",
    "        if word in [\"theories\",\"theory\",\"conspiracy\"]:\n",
    "            S[word] = \"conspiracy theory\"\n",
    "        if word == \"alex\" or word == \"jones\":\n",
    "            S[word] = \"alex jones\"\n",
    "        if word == \"trump\" or word == \"trumps\":\n",
    "            S[word] = \"donald trump\"\n",
    "        if word == \"virus\" or word == \"coronavirus\" or word == \"covid19\":\n",
    "            S[word] = \"coronavirus\"\n",
    "        if word == \"bill\" or word == \"gates\":\n",
    "            S[word] = \"bill gates\"\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "date_real = to_date(date)\n",
    "date_end_real = to_date(date_end)\n",
    "\n",
    "with open(results + \"/tfidfedNER25.txt\", \"r\") as f:\n",
    "    words = f.read().splitlines()\n",
    "\n",
    "counter = 0\n",
    "\n",
    "while date_real <= date_end_real:    \n",
    "    trajs = []\n",
    "    date_margin = create_range(date_real)\n",
    "\n",
    "    S_tfidf = create_dict(words[counter])\n",
    "    for key in S_use:\n",
    "        if key not in S_tfidf:\n",
    "            S_tfidf[key] = S_use[key]\n",
    "\n",
    "    S_tfidf=recheck(S_tfidf)\n",
    "    for d in date_margin:\n",
    "        ner_file = path + from_date(d) + \"/into_relex_relations_-1.csv\"\n",
    "        a = extract_traj(ner_file, S_tfidf, str(from_date(d)))\n",
    "        trajs.extend(a)\n",
    "        \n",
    "\n",
    "    plot_fig(trajs, from_date(date_real))\n",
    "    date_real += timedelta(days = 1)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "20200125000001\n"
    }
   ],
   "source": [
    "print(from_date(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Results0424/Communities/step0101.png'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-9ad78259dc4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnx_agraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_agraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'circo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/Communities/step\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pygraphviz/agraph.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, path, format, prog, args)\u001b[0m\n\u001b[1;32m   1516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_fh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w+b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m             \u001b[0mfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pygraphviz/agraph.py\u001b[0m in \u001b[0;36m_get_fh\u001b[0;34m(self, path, mode)\u001b[0m\n\u001b[1;32m   1545\u001b[0m                 \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bzcat \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# probably not portable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1546\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1547\u001b[0;31m                 \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1548\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'write'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;31m# Note, mode of file handle is unchanged.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Results0424/Communities/step0101.png'"
     ]
    }
   ],
   "source": [
    "communities = \"Results0424/communities.txt\"\n",
    "with open(communities, \"r\") as f:\n",
    "    list_lines = f.readlines()\n",
    "    for l in list_lines:\n",
    "        G=nx.Graph()\n",
    "        match  = re.findall(r'{(.+?)}', l)\n",
    "        G.add_nodes_from(match, shape='ellipse')\n",
    "        A = nx.nx_agraph.to_agraph(G)\n",
    "        A.layout('circo')\n",
    "        A.draw(results + \"/Communities/step\" + str(l[0:4]) + '.png')"
   ]
  }
 ]
}
{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bit4bd693e9780a48afbf8d298023c65066",
   "display_name": "Python 3.7.3 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "True\n"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "corpus = []\n",
    "vocabulary = []\n",
    "\n",
    "entity_file = \"NER_list.csv\"\n",
    "ent_file = pd.read_csv(entity_file)\n",
    "\n",
    "vocabulary = []\n",
    "for row in ent_file.iterrows():\n",
    "    try:\n",
    "        words = row[1]['entity'].split()\n",
    "    except:\n",
    "        words = [row[1]['entity']]\n",
    "    for word in words:\n",
    "        if word not in vocabulary:\n",
    "            vocabulary.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "ordered_list = []\n",
    "temp_corp_list = []\n",
    "window = 5\n",
    "\n",
    "for root, dirs, files in os.walk(\"ExtractedGDELTData\"):\n",
    "   for dirname in sorted(dirs):\n",
    "        ordered_list.append(\"ExtractedGDELTData/\" + dirname + \"/into_relex.txt\")\n",
    "\n",
    "for index in range(len(ordered_list[0:-(window - 1)])):\n",
    "    temp_corp_list.append(ordered_list[index:index + window])\n",
    "\n",
    "corpus = []\n",
    "for corp in temp_corp_list:\n",
    "    temp_corpus = \"\"\n",
    "    for cor in corp:\n",
    "        with open(cor, \"r\") as f:\n",
    "            mylist = f.read().splitlines()\n",
    "            for l in mylist:\n",
    "                if l != \"text\" and l != \"\\n\":\n",
    "                    temp_corpus += \" \" + l\n",
    "    corpus.append(temp_corpus)\n",
    "\n",
    "pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)),\n",
    "                 ('tfid', TfidfTransformer())]).fit(corpus)\n",
    "pipe['count'].transform(corpus).toarray()\n",
    "tfidf = pipe.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ners = 25\n",
    "top_actants = np.argsort(tfidf.toarray(), axis=1)[:,-1*top_ners:]\n",
    "\n",
    "with open(\"Results0423/tfidfedNER\" + str(top_ners) + \".txt\", \"a+\") as f:\n",
    "    for row in top_actants:\n",
    "        for r in row:\n",
    "            f.write(vocabulary[r] + \", \")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Count of Articles:  1757\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "headers = requests.utils.default_headers()\n",
    "headers.update({ 'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0'})\n",
    "url = \"https://www.covid19-archive.com/all-articles-on-one-page/\"\n",
    "req = requests.get(url, headers)\n",
    "soup = BeautifulSoup(req.content, 'html.parser')\n",
    "gg = soup.find_all(\"a\")\n",
    "c = 0\n",
    "list_of_urls = []\n",
    "for g in gg:\n",
    "    string_link = g.get(\"href\")\n",
    "    if \"https://web.archive.org/web/\" in string_link:\n",
    "        list_of_urls.append(string_link)\n",
    "        c += 1\n",
    "print(\"Total Count of Articles: \", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /usr/local/lib/python3.7/site-packages/jieba/dict.txt ...\n",
      "Dumping model to file cache /var/folders/g0/hnbkmd414h51nyqlfy97813r0000gn/T/jieba.cache\n",
      "Loading model cost 1.3275260925292969 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Failed Links:  8\n"
     ]
    }
   ],
   "source": [
    "d = {'Author': [], 'Date': [], \"Text\": []}\n",
    "lost = 0\n",
    "for art in list_of_urls:\n",
    "    try:\n",
    "        article = Article(art)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        d['Author'].append(', '.join(article.authors))\n",
    "        d['Date'].append(str(article.publish_date))\n",
    "        d['Text'].append(article.text)\n",
    "    except:\n",
    "        lost += 1\n",
    "    \n",
    "raw_text = pd.DataFrame(data=d)\n",
    "raw_text.to_csv(\"covid19rawDataset.csv\")\n",
    "print(\"Number of Failed Links: \", lost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text[raw_text.Text != '']\n",
    "raw_text.to_csv(\"covid19rawDataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"covid19rawDataset.csv\")\n",
    "\n",
    "import re\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "digits = \"([0-9])\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "        \n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "    \n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "        \n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    \n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"into_relex.txt\", \"a+\") as file_object:\n",
    "    file_object.write(\"text\\n\")\n",
    "    for _,row in data.iterrows():\n",
    "        text = split_into_sentences(str(row[\"Text\"]))\n",
    "        one_article = ' '.join(text)\n",
    "        file_object.write(one_article + \"\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
